{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Cost Prediction - Litigation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 150 #TPPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(TP,df):\n",
    "    #Creating a dictionary recording all the postions from EMPL_UNO and POSITIONS\n",
    "    df_pos = dict(list(zip(df.EMPL_UNO.astype(str), df.POSITION)))\n",
    "    df = df.replace(' ', np.nan)\n",
    "    df = df.replace(' .', np.nan)\n",
    "    #import pickle files\n",
    "    objects = []\n",
    "    with (open(\"C://python working//Dissertation Project//pos_dict.pkl\", \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "    with (open(\"C://python working//Dissertation Project//action_dict_updated.pkl\", \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "    with (open(\"C://python working//Dissertation Project//task_dict_updated.pkl\", \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "    with (open(\"C://python working//Dissertation Project//task_phase.pkl\", \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    # Mapped POSITION values from pickle files\n",
    "    df.replace({'POSITION': objects[0]}, inplace = True)\n",
    "    # Mapped ACTION_DESC values from pickle files\n",
    "    df.replace({'ACTION_DESC': objects[1]}, inplace = True)\n",
    "    # Mapped ACTION_DESC values from pickle files\n",
    "    df.replace({'PHASE_DESC': objects[2]}, inplace = True)\n",
    "    # Mapped ACTION_DESC values from pickle files\n",
    "    df.replace({'TASK_DESC': objects[3]}, inplace = True)\n",
    "    # Replace all the NaN values in PHASE_DESC and TASK_DESC by Missing values\n",
    "    df['PHASE_DESC'].fillna('Missing', inplace=True)\n",
    "    df['TASK_DESC'].fillna('Missing', inplace=True)\n",
    "    #Creating main datatable according to unique ID number\n",
    "    df_main = pd.DataFrame(df['MATTER_UNO'].unique())\n",
    "    #Rename column\n",
    "    df_main.rename(columns={0: 'MATTER_UNO'}, inplace = True)\n",
    "    # Creating TOTAL_COST column\n",
    "    df_cost = df[['MATTER_UNO','BILLED_AMT']].groupby(['MATTER_UNO']).sum().rename(columns={'BILLED_AMT': 'TOTAL_AMT'})\n",
    "    # #Total cost of each matter\n",
    "    # df_cost\n",
    "    #Merge total cost of each matter to main table\n",
    "    df_main = df_main.merge(df_cost, on='MATTER_UNO', how = 'left')\n",
    "    # We have 236 matters are 0 cost in total, which can be dropped.\n",
    "    df_main = df_main.drop(df_main[df_main.TOTAL_AMT==0].index).reset_index().drop(columns = 'index', axis=1)\n",
    "    # Create VALUE_RANGE variable and check the number of people in each range\n",
    "    df_main['VALUE_RANGE'] = pd.DataFrame(pd.qcut(df_main['TOTAL_AMT'], 4,labels = False))\n",
    "    # Check the total matter numbers in different VALUE_RANGE\n",
    "    df_val = df_main[['MATTER_UNO','VALUE_RANGE']].groupby('MATTER_UNO').mean().groupby('VALUE_RANGE').size()\n",
    "    # Creating DAYS_SPENT\n",
    "    df['DAYS_SPENT'] = pd.to_datetime(df['CLOSE_DATE']) - pd.to_datetime(df['OPEN_DATE'])\n",
    "    # get the earliest DATE among all the TRAN_DATE to each matter\n",
    "    df_start = df[['MATTER_UNO','TRAN_DATE']].groupby('MATTER_UNO').min().rename(columns={'TRAN_DATE':'FIRST_TRAN_DATE'})\n",
    "    df = df.merge(df_start, on='MATTER_UNO', how = 'left')\n",
    "    #Get the earliest date between FIRST_TRAN_DATE and OPEN_DATE as the START_DATE to eachmatter\n",
    "    df_start = df[['MATTER_UNO','TRAN_DATE', 'FIRST_TRAN_DATE']].groupby('MATTER_UNO').min()\n",
    "    # Merge the earliet TRAN_DATE AS START_DATE to the datetable\n",
    "    df_start=df_start.rename(columns={'TRAN_DATE' : 'START_DATE'})\n",
    "    df = df.merge(df_start, on='MATTER_UNO', how = 'left')\n",
    "    # Drop the Dates used to find the earliest start_date\n",
    "    df.drop(columns = ['FIRST_TRAN_DATE_x', 'FIRST_TRAN_DATE_y'], inplace = True)\n",
    "    # Creating new DAYS_SPENT, as the days spent should be the one from ealiest START_DATE to final CLOSE_DATE.\n",
    "    df['DAYS_SPENT'] = pd.to_datetime(df['CLOSE_DATE']) - pd.to_datetime(df['START_DATE'])\n",
    "    # Convert days to numerical number.\n",
    "    df['DAYS_SPENT'] = df['DAYS_SPENT'].astype('timedelta64[D]')\n",
    "    # Firstly, Merge DAYS_SPENT to each matter to main table\n",
    "    df_days = df[['MATTER_UNO','DAYS_SPENT']].groupby('MATTER_UNO').mean()\n",
    "    df_main = df_main.merge(df_days, on='MATTER_UNO', how = 'left')\n",
    "    # Define MAPE (Mean absolute percentage error) to check regression accuracy\n",
    "    def mean_absolute_percentage_error(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    # Save all matters more than TP days as we would need time length periods more than TP days\n",
    "    df_main = df_main[df_main['DAYS_SPENT'] >= TP]\n",
    "    #Creating DAYS_TO_START feature to indicate days from START_DATE.\n",
    "    df['DAYS_TO_START'] = pd.to_datetime(df['TRAN_DATE']) - pd.to_datetime(df['START_DATE'])\n",
    "    # Convert days to numerical number.\n",
    "    df['DAYS_TO_START'] = df['DAYS_TO_START'].astype('timedelta64[D]')\n",
    "    #Drop all the othe DATE related columns as they will not be put into model\n",
    "    df.drop(columns = ['TRAN_DATE', 'POST_DATE', 'OPEN_DATE', 'CLOSE_DATE', 'START_DATE'], inplace = True)\n",
    "    #Save all the entries within time periods from the start of the matter TP\n",
    "    df = df[df['DAYS_TO_START'] <= TP]\n",
    "    #Creating PAST_ENGAGEMENT columns\n",
    "    df['PAST_ENGAGEMENT'] = df['CLNT_MATT_CODE'].astype(str).str[-1].astype(int)\n",
    "    df_past = df[['MATTER_UNO', 'PAST_ENGAGEMENT']].groupby('MATTER_UNO').mean()\n",
    "    #Merge PAST_ENGAGEMENT columns to main table\n",
    "    df_main = df_main.merge(df_past, on='MATTER_UNO', how = 'left')\n",
    "    #Creating NOW_ENGAGEMENT columns within time periods\n",
    "    df_now = pd.DataFrame(df[['MATTER_UNO','TIME_UNO']].groupby('MATTER_UNO').size()).rename(columns={0:'NOW_ENGAGEMENT'})\n",
    "    #Merge NOW_ENGAGEMENT columns to main table\n",
    "    df_main = df_main.merge(df_now, on='MATTER_UNO', how = 'left')\n",
    "    #Creating PRE_TIMES columns within time periods\n",
    "    df['PHASE_DESC'] = df['PHASE_DESC'].str.upper()\n",
    "    df_PT = pd.DataFrame(df[df['PHASE_DESC'].str.contains('PRE-ACTION')].groupby('MATTER_UNO')\n",
    "                         .size()).rename(columns = {0:'PRE_TIMES'})\n",
    "    #Merge PRE_TIMES columns to main table\n",
    "    df_main = df_main.merge(df_PT, on='MATTER_UNO', how = 'left')\n",
    "    #Fill NULL values with mean in PRE_TIMES columns\n",
    "    df_main['PRE_TIMES'].fillna(0, inplace=True)\n",
    "    #Creating PRE_COST columns\n",
    "    df_bill = df[['MATTER_UNO', 'TIME_UNO', 'BILLED_AMT']]\n",
    "    #Sum up all the BILLED_AMT wihtin time periods\n",
    "    df_bill = df_bill[['MATTER_UNO','BILLED_AMT']].groupby('MATTER_UNO').sum().rename(columns={'BILLED_AMT': 'PRE_COST'})\n",
    "    #Merge the PRE_COST to main table\n",
    "    df_main = df_main.merge(df_bill, on = 'MATTER_UNO', how = 'left')\n",
    "    #Creating NUM_PPL\n",
    "    #ц│ицДПхПпшГ╜ф╝ЪцЬЙ collinarity хЫаф╕║ф╕НхРМчЪДpositionф║║цХ░хКаш╡╖цЭех░▒чнЙф║ОNUM_PPL\n",
    "    df_num = pd.DataFrame(df[['MATTER_UNO','EMPL_UNO']].groupby('MATTER_UNO')['EMPL_UNO'].nunique())\n",
    "    df_num.rename(columns={'EMPL_UNO' : 'NUM_PPL'}, inplace = True)\n",
    "\n",
    "    #Merge to main table\n",
    "    df_main = df_main.merge(df_num, on='MATTER_UNO', how ='left')\n",
    "    #Merge MATT_DEPT_NAME to main table\n",
    "    df_dept = df[['MATTER_UNO', 'MATT_DEPT_NAME']]\n",
    "    #Drop dupliactes\n",
    "    df_dept.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "    #Merge into the main table\n",
    "    df_main = df_main.merge(df_dept, on = 'MATTER_UNO', how = 'left')\n",
    "    # One hot encoding MATT_DEPT_NAME column\n",
    "    lb = LabelBinarizer()\n",
    "    # Join the encoded df\n",
    "    df_main = df_main.join(pd.DataFrame(lb.fit_transform(df_main['MATT_DEPT_NAME']),\n",
    "                              columns=lb.classes_, \n",
    "                              index=df_main.index))\n",
    "    # Drop one MATT_DEPT_NAME Column\n",
    "    df_main.drop(columns = 'MATT_DEPT_NAME', inplace = True)\n",
    "    # Creating features about all the different positions responsible for the matter within time periods. TP\n",
    "    df_po = pd.DataFrame(df[['MATTER_UNO','EMPL_UNO']].groupby('MATTER_UNO')['EMPL_UNO'].unique()).reset_index()\n",
    "    #Creating vector of different positions for each matter\n",
    "    df_test = pd.DataFrame(df_po.EMPL_UNO.tolist(), index = df_po.MATTER_UNO)\n",
    "    df_test.reset_index(inplace=True)\n",
    "    df_test = df_test.fillna(0).applymap(int).applymap(str)\n",
    "    # Convert unique EMPL_UNO of each matter to positons.\n",
    "    df_test.replace({0: df_pos, 1: df_pos, 2: df_pos, 3: df_pos, 4: df_pos, 5: df_pos, 6: df_pos, 7: df_pos,\n",
    "                     8: df_pos, 9: df_pos, 10: df_pos, 11: df_pos, 12: df_pos, 13: df_pos, 14: df_pos, 15: df_pos,\n",
    "                     16: df_pos, 17: df_pos, 18: df_pos, 19: df_pos, 20: df_pos, 21: df_pos, 22: df_pos, 23: df_pos,\n",
    "                     24: df_pos,  25: df_pos}\n",
    "                   , inplace = True)\n",
    "    # Creating position categories according to the meeting with supervisor\n",
    "    Others = [ 'PSL', 'Costs',   'Compliance', 'Others (not billing)', 'Outdoor clerk', 'Consultant', 'Knowledge Services', 'EA & Projects Advisor', 'Secondee', 'Knowledge Analyst', 'eDisclosure', 'nan',  'Cyber', 'Senior Knowledge Analyst',   'Business Development Manager','Business Insight Manager',   'Senior Data Protection Specialist',  'Costs Lawyer',  'Consultant', 'Sports Lawyer',  'Knowledge Manager', 'Trainee Solicitor 2nd year', 'Trainee Solicitor (2nd yr)',  'Legal Executive', 'Knowledge Solutions Analyst', 'Project Manager', 'Legal Administrator',  'File Closing Support', 'Professional Support Lawyer', 'E-Disclosure Project Manager', 'Barrister', 'eDisclosure Project Manager',  'Cyber Inteligence Analyst',  'Cyber Intelligence Analyst', 'Cyber Innovation Architect',  'Cyber Manager',  'Trainee 2nd year', 'Cyber Developer', 'Junior Solutions Engineer', 'Intern', 'Temp', 'Senior Risk & Compliance Manager']\n",
    "    Trainee = ['Trainee 2nd Year', 'Trainee 1st Year', 'Junior Solutions Engineer', 'Intern',  'Trainee Solicitor (2nd Year)',  'Trainee (Secondment)', 'Trainee']\n",
    "    ParalegalPPA = ['PPA', 'Paralegal', 'Trade Mark Paralegal', 'Paralegal\\\\PA', 'Legal Assistant',  'Assistant Solicitor', 'Paralegal PA', 'Personal Assistant', 'Legal assistant', 'Paralegal/Personal Assistant',  'PA', 'Trade Mark Attorney',  'Litigation Recoveries Assistant']\n",
    "    Associate = ['Indian Qualified Associate',  'Associate',  'In house compliance associate', 'Senior Associate',  'Associate (also BVI qualified)', 'Associate (Member of the New York Bar)', 'Associate (Barrister)', 'Associate, Chartered Trade Mark Attorney', 'Indian Qualified Associate']\n",
    "    ManagingAssociate = ['Managing Associate, PSL', 'Managing Associate', 'Managing Associate (also BVI qualified)', 'Professional Support Lawyer - Managing Associate', 'Managing Associate, Professional Support Lawyer', 'Managing Associate (Russian qualified lawyer)', 'Managing Associate (Admitted at the Austrian Bar)']\n",
    "    LegalDirector = ['Legal Director, PSL', 'Legal Director', 'Legal Director (Barrister)',   'Head of e Discovery',  'Head of Employment Policy & Engagement', 'Head of Legal Operations', 'China Desk Lead',  'Managing Associate (Russian qualified lawyer)',  'Professional Support Lawyer - Legal Director', 'Head of Legal Operations', 'Cyber Consulting Director', 'Cyber Intelligence Director', 'Head of Employment Policy & Engagement']\n",
    "    Partner = ['Partner', 'Senior Equity Partner', 'Equity Partner', 'Partner (Junior)', 'General Counsel, Partner and COLP', 'Deputy Chairman', 'Senoir Equity Partner']\n",
    "    # Replace all the positions with dictionary above\n",
    "    df_test.replace(dict.fromkeys(Others, 'Others'), inplace = True)\n",
    "    df_test.replace(dict.fromkeys(Trainee, 'Trainee'), inplace = True)\n",
    "    df_test.replace(dict.fromkeys(ParalegalPPA, 'ParalegalPPA'), inplace = True)\n",
    "    df_test.replace(dict.fromkeys(Associate, 'Associate'), inplace = True)\n",
    "    df_test.replace(dict.fromkeys(ManagingAssociate, 'ManagingAssociate'), inplace = True)\n",
    "    df_test.replace(dict.fromkeys(LegalDirector, 'LegalDirector'), inplace = True)\n",
    "    df_test.replace(dict.fromkeys(Partner, 'Partner'), inplace = True)\n",
    "    # Build up table to calculate all the positions in each matter\n",
    "    df_pnum = df_test.apply(pd.Series.value_counts, axis=1)[['Others', 'Trainee', 'ParalegalPPA', \n",
    "                                                   'Associate','ManagingAssociate', 'LegalDirector', 'Partner']].fillna(0)\n",
    "    # Join the table above to get the positions for each matter\n",
    "    df_po_num = df_test.join(df_pnum)\n",
    "    df_po_num = df_po_num[['MATTER_UNO', 'Others','Trainee','ParalegalPPA','Associate','ManagingAssociate','LegalDirector','Partner']]\n",
    "    df_po_num = df_po_num.applymap(int)\n",
    "    # Merge back to the main table\n",
    "    df_main = df_main.merge(df_po_num, on = 'MATTER_UNO', how = 'left')\n",
    "    # Drop all the matters from support department according to the communication with supervisor\n",
    "    df_main.drop(df_main[df_main['Support']==1].index, inplace= True)\n",
    "    # Classification\n",
    "    df_main_cla = df_main.drop(['MATTER_UNO','TOTAL_AMT','DAYS_SPENT'], axis=1)\n",
    "    # One Hot encoding the Value_RANGE\n",
    "    encoder = LabelEncoder()\n",
    "    cat = df_main_cla['VALUE_RANGE']\n",
    "    cat_encoded = encoder.fit_transform(cat)\n",
    "    encoder = OneHotEncoder()\n",
    "    df_main_1hot = encoder.fit_transform(cat_encoded.reshape(-1,1))\n",
    "    df_main_1hot = pd.DataFrame(df_main_1hot.toarray()).applymap(int)\n",
    "    #Creat arrays for the features and the response variable\n",
    "    y = df_main_cla['VALUE_RANGE'].values\n",
    "    X = df_main_cla.drop('VALUE_RANGE', axis = 1).values\n",
    "    # Create new training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    # Create the classifier: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=7)\n",
    "    knn.fit(X_train, y_train)\n",
    "    score = knn.score(X_test, y_test)\n",
    "    print(f\"For {TP} days, KNN Model score is {score * 100:.2f}%\")\n",
    "    model = DummyClassifier(strategy='most_frequent')\n",
    "    # fit model\n",
    "    cv_results = cross_val_score(model, X, y, cv =5)\n",
    "    print(cv_results)\n",
    "    print(\"Accuracy of Model with Cross Validation is:\",np.mean(cv_results) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-38e50a865db0>:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dept.drop_duplicates(subset=None, keep='first', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 30 days, KNN Model score is 51.08%\n",
      "[0.25179856 0.25270758 0.25270758 0.25270758 0.24909747]\n",
      "Accuracy of Model with Cross Validation is: 25.180375555151546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-38e50a865db0>:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dept.drop_duplicates(subset=None, keep='first', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 60 days, KNN Model score is 61.23%\n",
      "[0.25       0.25090909 0.25090909 0.25090909 0.24727273]\n",
      "Accuracy of Model with Cross Validation is: 25.000000000000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-38e50a865db0>:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dept.drop_duplicates(subset=None, keep='first', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 90 days, KNN Model score is 68.77%\n",
      "[0.25650558 0.25746269 0.25746269 0.25746269 0.25746269]\n",
      "Accuracy of Model with Cross Validation is: 25.7271264495367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-38e50a865db0>:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dept.drop_duplicates(subset=None, keep='first', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 120 days, KNN Model score is 79.92%\n",
      "[0.26640927 0.26640927 0.26640927 0.26744186 0.26744186]\n",
      "Accuracy of Model with Cross Validation is: 26.682230403160634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-38e50a865db0>:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dept.drop_duplicates(subset=None, keep='first', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 150 days, KNN Model score is 82.11%\n",
      "[0.2804878  0.27642276 0.27642276 0.27642276 0.27642276]\n",
      "Accuracy of Model with Cross Validation is: 27.723577235772357\n"
     ]
    }
   ],
   "source": [
    "# Testing 30, 60, 90 ,120, 150 days results of KNN model\n",
    "for TP in range(30,151,30):\n",
    "    accuracy(TP,df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
